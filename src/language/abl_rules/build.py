''' This file generates the rules that are used by the lexer to tokenize the code. 
    Any changes to the keywords should be made in _keywords.py

    The rules are generated from the keywords in _keywords.py.

    The final abl.py file is used by the lexer to tokenize the code. It's a real pain to 
    make changes in the abl.py file due to it's size. So this file reduces the mental 
    overhead, and prevent mistakes.

    The files are generated by running this file manually with python and placing 
    the output in the language folder. I don't want to make this easier since 
    it has the change to make difficult to recognize bugs.  

    There may be faster ways to do this, but it gets run once so maintainability
    should be the focus. 
'''
from _keywords import keyword_master 
from __init__ import colors, default_color
import os

### 
# Build settings
### 

# Going to track white space for dynamic syntax highlighting 
# the 
track_whitespace = True
dev_colors = False

###
# Dev logging
###

indent_logger = 'logger.verbose(t)' if dev_colors else ''


# Reset the palette if working in dev mode
if dev_colors:
    for k in colors:
        colors[k] = ''

###
# Construction functions
###
def build_regex(full_word:str, abr:str=None) -> str:
    ''' Build a regular expression to match anything up to the abbreviation '''
    # The multiple occurances of replace are required since we are truncating
    # the original word length not the length with escapes. 

    # Starts the expression
    reg = r'\b(?:' + full_word.replace('-','\-').replace('&', '\\&').replace(' ', r'\s')
    
    # Allow for not having the abbreviation
    if abr != None:
        # Build it backwards so that the longest match is found first
        for i in range(len(abr), len(full_word))[::-1]:
            reg += r'|'+ full_word[:i].replace('-','\-').replace('&', '\\&').replace(' ', r'\s')
    
    # Close off the expression
    reg += r')(?!\-)\b'
    return reg

def write_lookup_file() -> None:
    ''' The lookup file is used to match most generic keywords. 
    
        Things that are not in the lookup and have functions are:
        - Preprocessor directives
        - Block start and end keywords
    '''
    kw_lookup_file = open(os.path.join('abl_lookup.py'), 'w')
    kw_lookup_file.write( \
'''#This file was generated by src/models/abl_rules/build.py
# Do not edit this file directly.
# If you want to change the keywords, edit src/models/abl_rules/_keywords.py
# If you want to change the colors, edit src/models/abl_rules/config.py

kw_lookup = {
    ''')

    lookup_list = [ k for k in keyword_master if k['abr'] is None and not k['starts_block'] and not k['ends_block'] ]
    # Preprocessors need to be handled on their own
    lookup_list = [ k for k in lookup_list if k['keyword'][0] != '&' ]
    # Abbreviations need to be handled on their own
    lookup_list = [ k for k in lookup_list if k['abr'] is None ]

    for keyword in lookup_list:
        # Add the color to the list
        keyword['tag'] += [colors.get(keyword['cat'], default_color)]
        # Consolidate the keyword dictionary to only the values we need
        new_dict = {key: v for key, v in keyword.items() if key in ['keyword', 'token', 'cat', 'tag', 'mark']}
        
        # This was removed from the the keyword_master list but am leaving it
        # here in case there are other instances of this.,
        if keyword['keyword'] == "'":
            kw_lookup_file.write(f'    "{keyword["keyword"]}": {new_dict}, \n')
        else:
            kw_lookup_file.write(f"    '{keyword['keyword']}': {new_dict}, \n")
    # Close up the dictionary and the file. 
    kw_lookup_file.write('}')
    kw_lookup_file.close()

def build_tokens() -> list:
    ''' Takes the keyword lists and builds the tokens for the lexer '''
    tokens = []
    # These are the custom tokens which are not in the keywords list and may be manually added
    # Rules defined with functions do not need tokens added.
    punctuation = ['COMMA', 'SEMICOLON', 'LBRACKET', 'RBRACKET', 'LBRACE', 'RBRACE', 'TILDE']
    operators = ['PLUS', 'MINUS', 'MULTIPLY', 'DIVIDE', 'GTEQ', 'LTEQ']
    special = ['UNKNOWN', 'WHITESPACE']
    
    tokens += punctuation + operators + special

    # Remove any keywords that get their own function. 
    tokens_list = [ k for k in keyword_master if k['abr'] is None and not k['starts_block'] and not k['ends_block'] ]
    # Preprocessors get their own function
    tokens_list = [ k for k in tokens_list if k['keyword'][0] != '&' ]
    # Abbreviations get their own function
    tokens_list = [ k for k in tokens_list if k['abr'] is None ]

    # Add the keyword tokens
    tokens += [k['token'] for k in tokens_list]
    return tokens


def generate_indenting_token_functions() -> str:
    ''' Automatically generate the functions that handle indenting and dedenting '''

    indenting_code = ''

    ## Build the start and stop blocks words here
    start_or_stop = [ kw for kw in keyword_master if kw['starts_block'] or kw['ends_block']]

    # We want to check longest words first else they will match to shorter 
    # versions first. FRAME vs FRAME-NAME 
    start_or_stop.sort(key=lambda x: len(x['keyword']), reverse=True)

    filtered_keywords = [ kw for kw in start_or_stop if not kw['keyword'] in ['CLASS']]

    for kw in filtered_keywords:
        # Get the syntax highlighting
        if dev_colors:
            tag = 'magenta'
        else:
            tag = colors.get(kw["cat"], default_color)
        
        # Build out marks if there are any 
        if len(kw['mark']) != 0:
            mark = f"\n    t.mark = '{','.join(kw['mark'])}'"  
        else: 
            mark = ''

        # Sort out if soft block
        if kw['starts_block'] and not kw['hard_block']:
            soft = True
        else:
            soft = False

        # Build the hard block variables
        if kw['hard_block'] and not kw['ends_block']:
            hard = '\n    t.lexer.hard_block = True'
        elif kw['ends_block']:
            hard = '\n    t.lexer.hard_block = False'
        else:
            hard = ''

        # Flag if its in a query
        if kw['in_query']:
            query = '\n    t.lexer.in_query = True'
        else:
            query = ''

        # Build the indentation variables
        if kw['starts_block'] and kw['hard_block']:
            indent = "\n    if not t.lexer.soft_block: t.indent = 1"
        elif kw['starts_block']:
            indent = "\n    if not t.lexer.soft_block: t.indent = 1"
        elif kw['ends_block']:
            indent = "\n    t.indent = -99"

        # And most importantly, build the regex
        regex = build_regex(kw['keyword'], kw['abr'])

        logger = ''
        if dev_colors:
            logger = f'\n    {indent_logger}'

        indenting_code += f'''
def t_{kw['token']}(t):
    r'{regex}'{hard}{query}{mark}
    t.tag = '{tag}'{indent}
    t.lexer.soft_block = {soft}{logger}
    return t
    '''
        # End for loop
    return indenting_code


def generate_abbreviated_functions() -> str:
    ''' Automatically generate the functions that handle abreviated keywords '''
    ## Build the abbreviated words here
    abr = [ kw for kw in keyword_master if kw['abr'] != None and not kw['starts_block'] and not kw['ends_block']]

    # Remove some custom ones out
    remove_list = ['_GLOBAL_DEFINE']
    abr = [ kw for kw in abr if not kw['token'] in remove_list ]

    # Sort by size to prevent shorter words from matching first
    abr.sort(key=lambda x: len(x['keyword']), reverse=True)

    abbreviated_functions_string = ''
    for kw in abr:
        # Get the syntax highlighting
        tag = colors.get(kw["cat"], default_color)
        
        # Build out marks if there are any 
        if len(kw['mark']) != 0: 
            mark = f"\n    t.mark = '{','.join(kw['mark'])}'"
        else: 
            mark = ''

        # Build the regex
        regex = build_regex(kw['keyword'], kw['abr'])

        # Text not indented to write to file properly
        abbreviated_functions_string += f'''
def t_{kw['token']}(t):
    r'{regex}'
    t.tag = '{tag}'{mark}
    return t
    '''
        # End for loop
    return abbreviated_functions_string








###############################################################################
###                    START BUILDING THE LANGUAGE RULES                    ###
###############################################################################
###############################################################################

# Build the token list for the lexer. 
tokens = build_tokens()

# Write a new lookup file
write_lookup_file()

# Generate the indenting functions
indenting_functions   = generate_indenting_token_functions()
abbreviated_functions = generate_abbreviated_functions()


language_file = open(os.path.join('abl.py'), 'w')

# Write the file comments and warning that its generated
language_file.write(f'''
# This was generated by src/language/abl_rules/build.py
# Do not edit this file directly.
# If you want to change the keywords, edit src/language/abl_rules/_keywords.py
# If you want to change the colors, edit src/language/abl_rules/config.py

# App level imports
from modules.logging import Log

# Language level imports
from .modules.lex import TOKEN
from .abl_lookup import kw_lookup

logger = Log(__name__)

# If false, this will not track whitespace and force the user to comply
# This is required for the language model wether True or False
track_whitespace = {track_whitespace}

# Wow, long list. 
tokens = {tokens}

# Regular expression rules for simple tokens
t_GTEQ      = r'>='
t_LTEQ      = r'<='
t_NE        = r'<>'
t_GT        = r'>'
t_LT        = r'<'
t_PLUS      = r'\+'
t_MINUS     = r'\-'
t_MULTIPLY  = r'\*'
t_DIVIDE    = r'\/'
t_COMMA     = r'\,'
t_SEMICOLON = r'\;'
t_ASSIGN    = r'\:='
t_UNKNOWN   = r'\?'
t_TILDE     = r'~'
t_LBRACKET  = r'\['
t_RBRACKET  = r'\]'
t_LBRACE    = r'\{{'
t_RBRACE    = r'\}}'

###                        
# Rules are executed top down 
# So grab the comments and strings first to prevent mistakes
###        

# If newline is not checked first then any comment or string might wipe out
# the line number counters correctness 
def t_newline(t):
    r'\\n+'

    # Preprocessor directives start with the ampersand and end with newline
    if t.lexer.preprocessor:
        t.lexer.preprocessor = False
        t.lexer.in_query = False

    t.lexer.lineno += len(t.value)
    t.lexer.colno = 0
    # This extra space is a fix for the fact that sometimes the : separates an object and method
    # and sometimes it starts a block. The need for no space around the first and a space around 
    # the second can be solved adding a space to before the newline. 
    t.value = ' ' + t.value
    t.tag = 'nl'
    return t

###
# Key-Commands. Not progress keywords, but same same
###

# Seems excessive, but it's what's needed. 
def t_FUNCTION_KEY(t):
    r"""(?:'([fF]1[0-2]|[fF][1-9])'|"([fF]1[0-2]|[fF][1-9])"|\\b([fF]1[0-2]|[fF][1-9])\\b)"""
    t.tag = '{ colors['key command'] }' 
    return t

###
# special things
###

def t_SNG_COMMENT(t):
    r'//.*'
    t.tag = '{ colors['comment'] }'
    return t

# r'/\*[\s\S]*?\*/'     # Zero nested
# r'/\*(?:(?:(?!\*/)[\s\S])*?(?:/\*(?:(?!\*/)[\s\S])*?(?:/\*(?:(?!\*/)[\s\S])*?\*/(?:(?!\*/)[\s\S])*?)*\*/(?:(?!\*/)[\s\S])*?)*\*/' # Triple nested
def t_MULTI_COMMENT(t):
    r'/\*(?:(?:(?!\*/)[\s\S])*?(?:/\*(?:(?!\*/)[\s\S])*?\*/(?:(?!\*/)[\s\S])*?)*)\*/' # Double nested
    t.lexer.lineno += t.value.count('\\n')
    t.tag = '{ colors['comment'] }'
    return t

# Comments that have started are going to be a challenge
def t_COMMENT_STARTED(t):
    r'/\*.*'
    t.tag = '{ colors['comment'] }'
    return t

def t_CURLY_BRACE(t):
    r'{{(?:.*?)}}'
    t.tag = '{ colors['method'] }'
    return t

def t_NESTED_CURLY_BRACE(t):
    r'(?:\{{(?:[^{{}}]*\{{[^{{}}]*\}}[^{{}}]*)*[^{{}}]*\}})'
    t.tag = '{ colors['method'] }'
    return t

def t_ARRAY_BRACE(t):
    r'\[[\s\S]*?\]'
    t.tag = '{ colors['yellow']}'
    return t

def t_DATE_STR(t):
    r'(?:0?[1-9]|1[0-2])[/\-\.](?:0?[1-9]|[12][0-9]|3[01])[/\-\.](?:\d{{4}}|\d{{2}})'
    t.tag = '{ colors['string']}'
    return t

# Decimal must come before int or 0.5 will be int 0 and float .5
def t_DEC_STRING(t):
    r'\\b-?\d*\.\d+\\b'
    t.value = t.value
    t.tag = '{ colors['data type']}'
    return t

def t_INT_STRING(t):
    r'\\b-?\d+\\b'
    t.tag = '{ colors['data type']}'
    return t

def t_DBL_STRING(t):
    r'\\"[^"]*\\"'
    t.tag = '{ colors['string']}'
    return t

def t_SNG_STRING(t):
    r"\\'[^']*\\'"
    t.tag = '{ colors['string']}'
    return t

def t_COMPARISON_OP(t):
    r'\\b(?:GE|LE|GT|LT)\\b'
    t.tag = '{ colors['operator']}'
    return t

    
###
# Indentation custom rules
# These are mainly here to deal with the multiple indent initializers can exist 
# in a single line. Until the grammar is sorted out. This will be the work around
# These are here also incase they are needed 
#    cBlockEndStatements = "END.,

### Start indents ###

def t_CLASS(t):
    r'CLASS'
    # Leave extra room incase an extra space
    if t.colno < 5: 
        t.lexer.hard_block = True
        t.tag = '{ colors['define'] }'
        t.indent = 1
        t.lexer.soft_block = False
        { indent_logger }
    else:
        t.tag = '{ colors['data type'] }'
    return t

def t_CLASS_GETTER(t):
    r'GET\([a-zA-Z0-9\.\-\_\:]*\)'
    t.lexer.hard_block = True
    t.tag = '{ colors['define'] }'
    t.indent = 1
    t.lexer.soft_block = False
    { indent_logger }
    return t   

def t_FOR_EACH(t):
    r'\\bFOR\sEACH\\b'

    t.tag = '{ colors['define'] }'
    if not t.lexer.preprocessor:
        t.indent = 1
    # If in a query then we dont want to pick up indents from other keywords
        t.lexer.hard_block = True
        t.lexer.in_query = True
        { indent_logger }
        # This is mainly here in case of development. Once things are solid, this should be removed
        t.tag = '{ colors['define'] if not dev_colors else 'magenta' }'
    return t

## Procedurally generated the next large block of defines

# Starting with indent statements.
''')

###
# Add in previously generated indenting functions
###
language_file.write(indenting_functions)

language_file.write(f"""
###
# And now abbreviated keywords. Damn those abbreviated keywords. 
###

def t__GLOBAL_DEFINE(t):
    r'(?:\&GLOBAL\-DEFINE|\&GLOBAL\-DEFIN|\&GLOBAL\-DEFI|\&GLOBAL\-DEF|\&GLOBAL\-DE|\&GLOBAL\-D|\&GLOBAL\-|\&GLOBAL|\&GLOBA|\&GLOB)'
    
    # A global define could have a block start in it without an end. So start a hard block here 
    # and end it if we start another hard block. 
    t.lexer.preprocessor = True
    t.lexer.in_query = True
    t.tag = '{ colors['define'] }'
    return t
""")

language_file.write(abbreviated_functions)

language_file.write(f""" 
# This will track whitespace and adds a lot of time to processing. Not tracking 
# it however will force the user into the indent scheme
{'' if track_whitespace else '# ' }t_WHITESPACE = r'[\ \t]+'

# Some of these might be better not ignored. However for syntax highlighting
# they throw a lot of errors. As this is continuing, it might be better to 
# define 2 states, one for syntax highlighting and one for parsing.
# t_ignore = r'.'
t_ignore = '{ '' if track_whitespace else ' ' }}}\\t'

#####
### End of procedurally generated code
#####

# These are things like RUN dir/file.p(). And we need to keep the path together
def t_SRC_FILE(t):
    r'[a-zA-Z0-9][a-zA-Z0-9\.\-\/\_]*\.p[!a-zA-Z0-9]'
    t.tag = '{ colors['value']}'
    return t

# Catch all variable names etc. 
def t_ID(t):
    r'[a-zA-Z_][a-zA-Z_\:\-\.0-9]*[a-zA-Z_\%\-0-9]|\\b[a-zA-Z]\\b'
    _id = {{'keyword': '', 'token': 'ID', 'cat': '', 'tag': ['alt_blue'], 'mark': []}}
    result = kw_lookup.get(t.value.upper(),_id)    # Check for reserved words
    t.type = result['token']
    t.tag  = {"result['tag']" if not dev_colors else "''"}
    return t

### Should change this out if not following whitespace rules   
def t_PERIOD(t):
    r'\.'
    if t.lexer.soft_block:
        t.lexer.soft_block = False
        t.indent = -1
        t.tag = '{'magenta' if dev_colors else ''}'
    return t

def t_COLON(t):
    r':(?![a-zA-Z])'
    if not t.lexer.in_query and t.lexer.soft_block:
        t.lexer.soft_block = False
        t.tag = '{'magenta' if dev_colors else ''}'
    t.lexer.in_query = False
    return t

def t_EQUALS(t):
    r'='
    if not t.lexer.soft_block and not t.lexer.in_query:
        t.lexer.soft_block = True
        t.indent = 1
        t.tag = '{'magenta' if dev_colors else ''}'
    return t   

def t_LPAREN(t):
    r'\('
    t.indent = 1
    t.tag = '{'magenta' if not dev_colors else 'yellow'}'
    return t

def t_RPAREN(t):
    r'\)'
    t.indent = -1
    t.tag = '{'magenta' if not dev_colors else 'yellow'}'
    return t

# This is a hack to prevent the lexer from throwing an error on a bad word and deleting it.
def t_CATCHALL(t):
    r'[0-9][a-zA-Z_0-9\.,\-=]*'
    t.type = 'ERROR'
    t.tag = 'error'
    return t

def t_error(t):
    logger.error(f"Illegal character {{t.value[0:50]}} @ ln:{{t.lineno}} col:{{t.colno}} <<<<<<<<")
    return t
    ## We dont want to skip just yet. That is destructive and I don't trust this yet. 
    # t.lexer.skip(1)
""")

language_file.close()

print("FIN")